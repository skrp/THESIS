####################################################
# MKRX UNIX NETWORK

                                             ~ skrp
                                       Kehkay Genkai
                                      of the village
                                  Hidden in the 1337
{{  t_(o0)_j  }}
\\ Meri Kann //
_\\cibR punX//______________________________________
|
|
|
|
|
|
|
This manuscript details the machine of MKRX UNIX NETWORKS
This system connects nodes thru encrypted tunnels

SICC network file system
MKRX archive tools
HIVE work processors

KERN hardened FreeBSD_10.3 custom unix kernels
PSK-OPIE privilege partition & escalation

PROTO unix network protocol implementations
STAT node & network dtrace sensor analysis

ZFS diagrams PCI, SATA, SAS, USB
Schematics & Utilities

MAN manuals of instructions & examples
|
|
|
|
|
|
|
Let us preserve all insight
Till the last man walk

That he might not walk
In the darkness of past

Independent of time
Independent of wealth
Independent of government

In the order of Anarchy
In that way deliver basic rights

Files get lost
Storage gets easier
|
|
|
|
|
|
|
####################################################
# Table of Contents
####################################################

# 0 ################################################
SICC
  Standard
  Metadata
  Key
  SLICR
  BLKR

# 1 ################################################
MKRX
  Tools explained
  Tool use & examples
    create GET from BKUP
    create GET from UNIQ
    populate a new drive
    extract a source

# 2 ################################################
HIVE
  Structure
  Mechanics
    ORDER
    SLEEP
    SUICIDE
    CLEAN
    RESUME
  API
    SHA
    GET
    UNTAR
    REGX
    BLKR
  Logs

# 3 ################################################
KERN
  Hardened
  PSK-OPIE
  RINGS (rings of power)
  USR
    root
    sroot
    lord
    heir
    seer
    norm

# 4 ################################################
PSK-OPIE
  Review
  ASIGN
  Mine
    KEY
    RAW

# 5 ################################################
PROTO
  Overview
  SSH
  SCP
  FTP
  PF
  NFS
  IRC
  8080
  HTTPS
  MORSE

# 6 ################################################
STAT
  List
  dtrace
    cpu
    PID
    fs
    net
  Performance
    LOG
    ECHO
  ANAL
    SELF
    NET

# 7 ################################################
ZFS & Hardware
  ZFS
    0
    Mirror
    raidz2
    raidz3
  Hardware
    BOX
    BIOS
    cpu
    RAM
    PCI
    DEV
    Supply
    PCI
    SATA
    SAS
    USB
    JBOD

# 8 ################################################
SCHMATIC
  SIMP
  TOWR
  Utilities

# 9 ################################################
MAN
  unix manuals
    log
    mount
    boot
    ntfs
  perl manuals
    log
    language
    modules
  zfs manuals
    log
    hotswap
    import
    set
  dtrace manuals
    language

# END ##############################################
|
|
|
|
|
|
|
####################################################
# 0 - SICC
####################################################

######## System In Complete Chaos    ###############

# NAME SYSTEM ######################################
The core of this system is based on sha file computation

Sha is a unique file identifier that is the output of a mathematical algorithm
This mathematical algorithm will produce the same sha for the same file

Each file is named after its sha

This name system has two benefits:
  {1} de-duplicates files
  {2} assigns data into Chaos order

Sha uniqueness keeps all data:
  [-] simple
  [-] homogeneous
  [-] ultra-transient

The duplication of data is accounted in single-entry lists
Each data-pool is maximized in its ability to minimize the data-pool's disk-footprint

The duplication count allows for network backup prioritization to remote devices
This foundation is stable for automated backup management of massive-data networks

# METADATA #######################################
Metadata is kept isolated from the data
This isolation allows for the obfuscation of open data

Data-pool lists have lessened-value without Metadata
Exposure of lists can be nullified by salt

Only someone that has the metadata & access-key can request data

This file-system is built for ultra-transient network management
All files are located in the directory "/usr/nfs/pub/"

Two factors assist in obfuscation
  (1) No subdirectories to betray association
  (2) Alpha-numeric ordering of the hexadecimal names


Each file is periodically verified to have a corresponding metadata-file

Each metadata-file consists of 4 single-line entries:
  (1) name - XS (MKRX) standardized name
  (2) path - XS standardized path of extraction
  (3) size - number of bytes
  (4) encode - type of encoding

# KEY ##############################################
A KEY is a recipe to rebuild an obfuscated file

This allows for a powerful layer of unprecedented obfuscation
What is public gibberish is made private information thru access to a KEY file

Each KEY file is named after the original file sha
The KEY file contains sequential sha-per-line entries

Obfuscation Methods:

  [X] SLICR (secure against file-known)
        Shreds a file into random-sized parts & creates KEY
        Random-size prevents sha rainbow-tables of attacker with file-known

  [X] BLKR (insecure if file-known)
       Shreds file into standard-sized blocks & creates KEY
       Homogeneous data types will have duplicate-collisions which nullify the duplicate data
       Only one copy of data can exist in the data-pool
       Many KEY files can correspond to the same block reducing the data-pool disk-footprint
       The smaller block-size the greater power of compression & obfuscation of data-pool
       Each node configures own block-size

# END ##############################################

This file-system spread over a network has the following benefits:
  [o] All data easily accounted for in single-entry lists
  [o] Sane & Clean orders of massive-data on an network
  [o] Network level explicit control of the duplication of data
  [o] Each node serves what is possible
  [o] Each node has access to total data
  [o] Each node can assist in external backups

On a homogeneous unix kernel the network file-system is transparent
Only active tunnels will be displayed as available data-pools

Transparent file-system
  [l] store data remotely
  [l] get a remote file
  [l] serve a local file
  [l] backup network to local drive
|
|
|
|
|
|
|
####################################################
# 1 - MKRX
####################################################

######## MKRX Tools   ##############################

# SCRUB ############################################
Verify sha of the data is true
Report untrue data

# CHKMETA ##########################################
Confirm each file has metadata
Report rogue data

# UNIQ #############################################
Check if data is unique to the network
Compare NODE data against NET data
Create add backup duplicates to network

# XFR ##############################################
Get data onto local drive

# INDEX ############################################
Update metadata database hash dumps

# CLI ##############################################
Load INDEX into memory
Terminal Input & Output to produce single entry lists

Application Program Interface (API):
  {reset} reset array to full network-array
  {load}  load array from custom file
  {print} output current array
  {count} count array
  {value} output values of array 'name, path, size, encoding'
  {pop}   output size-specific lists that sum to a specific-amount of bytes

# EXAMPLES ########################################

Create GET for backup
  {TOTAL_NET}   location of network-wide file list
  {TOTAL_LOCAL} location of local file list
  {GET}         single-entry line lists of what TOTAL_LOCAL lacks from TOTAL_NET
+++++++++++++++++++++++++++++++++++++++++++++++++++
$usr@host> UNIQ TOTAL_NET TOTAL_LOCAL GET
+++++++++++++++++++++++++++++++++++++++++++++++++++

Create EXTRACT from UNIQ
  {TOTAL_SOURCE} sha lists of a source
  {TOTAL_NET} network-wide file list
  {EXTRACT} new unique files to network list
+++++++++++++++++++++++++++++++++++++++++++++++++++
$usr@host> UNIQ TOTAL_SOURCE TOTAL_NET EXTRACT
+++++++++++++++++++++++++++++++++++++++++++++++++++

Populate a new drive
  {~/INDEX/}         location of INDEX metadata database hash dumps
  {~/POP/1000000000} file to transfer
  {/mnt/USB/pool/}   location to dump files
+++++++++++++++++++++++++++++++++++++++++++++++++++
$usr@host> CLI ~/INDEX/
pop 1000000000
exit
+++++++++++++++++++++++++++++++++++++++++++++++++++
$usr@host> cd ~/POP/
+++++++++++++++++++++++++++++++++++++++++++++++++++
$usr@host> wc -l 1000000000
8493719 1000000000
+++++++++++++++++++++++++++++++++++++++++++++++++++
$usr@host> wc -l leftover_1000000000
39533365846 leftover_1000000000
+++++++++++++++++++++++++++++++++++++++++++++++++++
$usr@host> XFR ~/POP/1000000000 /mnt/USB/pool/
+++++++++++++++++++++++++++++++++++++++++++++++++++
$usr@host> CLI ~/INDEX/
load leftover_1000000000
pop 1000000000
exit
+++++++++++++++++++++++++++++++++++++++++++++++++++
Loop over & over by gigabyte-sized fragments

Extract a source
  {/mnt/bkup10/} location to grab new data
  {~/} location to dump data into ~/pool/ & ~/g/
+++++++++++++++++++++++++++++++++++++++++++++++++++
$usr@host> XS /mnt/bkup10/ ~/
+++++++++++++++++++++++++++++++++++++++++++++++++++
|
|
|
|
|
|
|
####################################################
# 2 - HIVE
####################################################

  HIVE
 DEMON - always online unix daemon goons with network API

  NODE Structure
 /tmp/$NAME/dump/ - DEMON specific host dump
 /tmp/PING - host DEMON roster

  NET Structure
 /HIVE/ - nfs remote mount
 /HIVE/PING - net DEMON roster
 /HIVE/node/$NODE - node specific directory
 /HIVE/node/$NODE/PID - node specific PID rooster
 /HIVE/node/$NODE/QUE/ - host specific API que
 /HIVE/cemetery/ - network wide DEMON graveyard
 /HIVE/cemetery/RAW_$NAME -
 /HIVE/cemetery/DONE_$NAME -
 /HIVE/cemetery/TODO_$NAME -

  Dump
 /$nfs_pool/ - remote que file dump
 /$nfs_g/ - remote que meta dump

  Mechanics
 ORDER
 SLEEP
 SUICIDE
 CLEAN
 RESUME

  API
 SHA - sha a file
 GET - http request URL & XS file into remote dump
 UNTAR - extract file archive type & XS into remote dump
 REGX - test massive-data for patterns
 BLKR - shred files into standard-blocks
 SLICR - shred files into random-blocks

  Logs


|
|
|
|
|
|
|
####################################################
# 3 - KERN
####################################################
|
|
|
|
|
|
|
####################################################
# 4 - PSKOPIE
####################################################
|
|
|
|
|
|
|
####################################################
# 5 - PROTO
####################################################
|
|
|
|
|
|
|
####################################################
# 6 - STAT
####################################################
|
|
|
|
|
|
|
####################################################
# 7 - ZFS & Hardware
####################################################
|
|
|
|
|
|
|
####################################################
# 8 - Schematics
####################################################
|
|
|
|
|
|
|
####################################################
# 9 - MAN
####################################################
|
|
|
|
|
|
|
